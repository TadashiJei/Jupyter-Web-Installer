{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog Image Generator using StyleGAN2\n",
    "\n",
    "This notebook demonstrates how to generate realistic dog images using a pre-trained StyleGAN2 model.\n",
    "StyleGAN2 is a state-of-the-art generative adversarial network for image synthesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, we'll install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install torch torchvision numpy matplotlib tqdm gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Pre-trained Model\n",
    "\n",
    "We'll download a pre-trained StyleGAN2 model for dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "# Create a directory for the model\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Download the pre-trained model for dogs\n",
    "url = 'https://drive.google.com/uc?id=1yjO5y2S0XA-p59Xkx9n8W9-KlsQxmLbm'\n",
    "output = 'models/stylegan2-afhqdog.pt'\n",
    "if not os.path.exists(output):\n",
    "    gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone StyleGAN2 Repository\n",
    "\n",
    "We need to clone the StyleGAN2-ADA repository to use its code for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
    "import sys\n",
    "sys.path.append('stylegan2-ada-pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dog Images\n",
    "\n",
    "Now we'll use the pre-trained model to generate dog images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the pre-trained model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Import required modules from the StyleGAN2 repository\n",
    "import dnnlib\n",
    "import legacy\n",
    "\n",
    "# Load the network\n",
    "network_pkl = 'models/stylegan2-afhqdog.pt'\n",
    "print(f'Loading networks from \"{network_pkl}\"...')\n",
    "with dnnlib.util.open_url(network_pkl) as f:\n",
    "    G = legacy.load_network_pkl(f)['G_ema'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to generate images\n",
    "def generate_images(num_images=5, seed=None):\n",
    "    # Set random seed for reproducibility if provided\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Generate random latent vectors\n",
    "    z = torch.randn(num_images, G.z_dim).to(device)\n",
    "    \n",
    "    # Generate images\n",
    "    with torch.no_grad():\n",
    "        img = G(z, None)\n",
    "    \n",
    "    # Convert images to numpy arrays\n",
    "    img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8).cpu().numpy()\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Generate and display some images\n",
    "images = generate_images(num_images=5, seed=42)\n",
    "\n",
    "# Plot the generated images\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(images[i])\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Generated Dog {i+1}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Save a Dataset of Dog Images\n",
    "\n",
    "Let's generate a larger dataset of dog images for machine learning purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Create a directory to save generated images\n",
    "output_dir = 'generated_dogs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Number of images to generate\n",
    "num_images = 100\n",
    "\n",
    "# Generate images in batches\n",
    "batch_size = 10\n",
    "num_batches = num_images // batch_size\n",
    "\n",
    "print(f\"Generating {num_images} dog images...\")\n",
    "\n",
    "for batch_idx in tqdm(range(num_batches)):\n",
    "    # Generate a batch of images\n",
    "    batch_images = generate_images(num_images=batch_size, seed=batch_idx)\n",
    "    \n",
    "    # Save each image in the batch\n",
    "    for i, img in enumerate(batch_images):\n",
    "        img_idx = batch_idx * batch_size + i\n",
    "        img_path = os.path.join(output_dir, f'dog_{img_idx:04d}.png')\n",
    "        \n",
    "        # Convert numpy array to PIL Image and save\n",
    "        Image.fromarray(img).save(img_path)\n",
    "\n",
    "print(f\"Successfully generated and saved {num_images} dog images to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation Between Dog Images\n",
    "\n",
    "We can also create smooth transitions between different dog images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def interpolate_images(num_steps=10):\n",
    "    # Generate two random latent vectors\n",
    "    z1 = torch.randn(1, G.z_dim).to(device)\n",
    "    z2 = torch.randn(1, G.z_dim).to(device)\n",
    "    \n",
    "    # Create interpolation steps\n",
    "    alphas = np.linspace(0, 1, num_steps)\n",
    "    interpolated_images = []\n",
    "    \n",
    "    # Generate images at each interpolation step\n",
    "    for alpha in alphas:\n",
    "        # Linear interpolation between the two latent vectors\n",
    "        z_interp = (1 - alpha) * z1 + alpha * z2\n",
    "        \n",
    "        # Generate image\n",
    "        with torch.no_grad():\n",
    "            img = G(z_interp, None)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8).cpu().numpy()[0]\n",
    "        interpolated_images.append(img)\n",
    "    \n",
    "    return interpolated_images\n",
    "\n",
    "# Generate interpolated images\n",
    "interpolated_images = interpolate_images(num_steps=10)\n",
    "\n",
    "# Plot the interpolation\n",
    "fig, axes = plt.subplots(1, 10, figsize=(20, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(interpolated_images[i])\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Step {i+1}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Mixing\n",
    "\n",
    "One of the cool features of StyleGAN2 is style mixing, where we can combine features from different dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def style_mixing(num_source=4, num_dest=3):\n",
    "    # Generate source and destination latent vectors\n",
    "    src_latents = torch.randn(num_source, G.z_dim).to(device)\n",
    "    dst_latents = torch.randn(num_dest, G.z_dim).to(device)\n",
    "    \n",
    "    # Maps from Z to W space\n",
    "    with torch.no_grad():\n",
    "        src_ws = G.mapping(src_latents, None)  # [NUM_SRC, num_ws, w_dim]\n",
    "        dst_ws = G.mapping(dst_latents, None)  # [NUM_DST, num_ws, w_dim]\n",
    "        \n",
    "        # Style layer indices to mix\n",
    "        # Low (0-3): Coarse features (pose, shape)\n",
    "        # Middle (4-8): Mid-level features (fur, ears, etc.)\n",
    "        # High (9+): Fine details (colors, textures)\n",
    "        mix_ranges = [[0, 3], [4, 8], [9, G.num_ws-1]]\n",
    "        src_images = []\n",
    "        mixed_images = []\n",
    "        \n",
    "        # Generate source images\n",
    "        for src_idx in range(num_source):\n",
    "            src_img = G.synthesis(src_ws[src_idx:src_idx+1], noise_mode='const')\n",
    "            src_img = (src_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8).cpu().numpy()[0]\n",
    "            src_images.append(src_img)\n",
    "        \n",
    "        # Generate mixed images for each destination and style range\n",
    "        for dst_idx in range(num_dest):\n",
    "            for mix_range in mix_ranges:\n",
    "                # Create a copy of the destination latent\n",
    "                w = dst_ws[dst_idx:dst_idx+1].clone()\n",
    "                \n",
    "                # Style mixing\n",
    "                for src_idx in range(num_source):\n",
    "                    # Apply source style to the specified range\n",
    "                    w_mixed = w.clone()\n",
    "                    w_mixed[:, mix_range[0]:mix_range[1]+1] = src_ws[src_idx:src_idx+1, mix_range[0]:mix_range[1]+1]\n",
    "                    \n",
    "                    # Generate mixed image\n",
    "                    img = G.synthesis(w_mixed, noise_mode='const')\n",
    "                    img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8).cpu().numpy()[0]\n",
    "                    mixed_images.append((src_idx, dst_idx, mix_range, img))\n",
    "    \n",
    "    return src_images, mixed_images\n",
    "\n",
    "# Generate source and mixed images\n",
    "src_images, mixed_images = style_mixing(num_source=4, num_dest=3)\n",
    "\n",
    "# Plot source images\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(src_images[i])\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Source {i+1}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot mixed images\n",
    "# We'll display 3x4x3 grid (3 destinations, 4 sources, 3 style ranges)\n",
    "mix_ranges = [[0, 3], [4, 8], [9, G.num_ws-1]]\n",
    "range_names = [\"Coarse\", \"Medium\", \"Fine\"]\n",
    "\n",
    "fig, axes = plt.subplots(3, 12, figsize=(24, 7))\n",
    "for dst_idx in range(3):\n",
    "    for range_idx, (mix_range, range_name) in enumerate(zip(mix_ranges, range_names)):\n",
    "        for src_idx in range(4):\n",
    "            # Find the corresponding mixed image\n",
    "            for mix_data in mixed_images:\n",
    "                if mix_data[0] == src_idx and mix_data[1] == dst_idx and mix_data[2] == mix_range:\n",
    "                    idx = dst_idx * 12 + range_idx * 4 + src_idx\n",
    "                    row, col = idx // 12, idx % 12\n",
    "                    axes[row, col].imshow(mix_data[3])\n",
    "                    axes[row, col].axis('off')\n",
    "                    if src_idx == 0:\n",
    "                        axes[row, col].set_title(f\"Dest {dst_idx+1}\\n{range_name}\")\n",
    "                    else:\n",
    "                        axes[row, col].set_title(f\"Src {src_idx}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
